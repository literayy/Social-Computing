import torch
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import os
from tqdm import tqdm
import random
from transformers import DebertaV2Tokenizer
# ==================== è§£å†³ä¸­æ–‡ä¹±ç é…ç½® ====================
plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows ç³»ç»Ÿä¼˜å…ˆä½¿ç”¨é»‘ä½“
plt.rcParams['axes.unicode_minus'] = False    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.family'] = 'sans-serif'


class RobustnessEvaluator:
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.model = self._load_trained_model()
        self.tokenizer = self._load_tokenizer()
        
        self.test_df = pd.read_csv(self.config.PREPROCESSED_COMBINED)
        # é‡‡æ ·æµ‹è¯•é›†
        self.test_df = self.test_df.sample(n=min(500, len(self.test_df)), random_state=42)
        
    def _load_tokenizer(self):
        """åŠ è½½DeBERTa tokenizer"""
        from transformers import DebertaV2Tokenizer
        return DebertaV2Tokenizer.from_pretrained(self.config.DEBERTA_PATH)
    
    def _load_trained_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„DomainAdaptiveDeBERTaæ¨¡å‹ï¼ˆé€‚é…æ–°çš„æ¨¡å‹ç»“æ„ï¼‰"""
        # å¯¼å…¥è®­ç»ƒå®šä¹‰çš„æ¨¡å‹ç±»
        from domain_adaptation import DomainAdaptiveDeBERTa  # å‡è®¾è®­ç»ƒä»£ç æ–‡ä»¶åä¸ºtrain_model.py
        
        model_path = os.path.join(self.config.MODEL_SAVE_PATH, 'best_model.pt')
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åŠ è½½æ¨¡å‹æƒé‡
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = DomainAdaptiveDeBERTa(self.config).to(self.device)
        
        # åŠ è½½çŠ¶æ€å­—å…¸ï¼ˆå…¼å®¹ä¸åŒçš„ä¿å­˜æ ¼å¼ï¼‰
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.eval()
        print(f"âœ“ åŠ è½½æ¨¡å‹: {model_path}")
        return model
    
    def adversarial_attack(self, text, attack_type='word_swap'):
        """
        ç”Ÿæˆæ–‡æœ¬å¯¹æŠ—æ ·æœ¬
        Args:
            text: åŸå§‹æ–‡æœ¬
            attack_type: æ”»å‡»ç±»å‹ ('word_swap', 'char_insert')
        Returns:
            å¯¹æŠ—æ ·æœ¬æ–‡æœ¬
        """
        if not isinstance(text, str) or len(text.strip()) == 0:
            return text
        
        if attack_type == 'word_swap':
            # éšæœºæ›¿æ¢10%çš„è¯
            words = text.split()
            if len(words) <= 1:
                return text
            
            num_swaps = max(1, len(words) // 10)
            swap_indices = random.sample(range(len(words)), min(num_swaps, len(words)))
            
            common_words = ['good', 'bad', 'great', 'nice', 'love', 'hate', 'like', 'think', 'very', 'so']
            for idx in swap_indices:
                words[idx] = random.choice(common_words)
            
            return ' '.join(words)
        
        elif attack_type == 'char_insert':
            # éšæœºæ’å…¥å­—ç¬¦
            chars = list(text)
            if len(chars) <= 1:
                return text
            
            num_inserts = max(1, len(chars) // 20)
            for _ in range(num_inserts):
                pos = random.randint(0, len(chars)-1)
                chars.insert(pos, random.choice('abcdefghijklmnopqrstuvwxyz '))
            
            return ''.join(chars)
        
        return text
    
    def test_cross_domain_accuracy(self):
        """è·¨åŸŸæ³›åŒ–æµ‹è¯•ï¼šCresciæµ‹Botï¼ŒGenderæµ‹Gender"""
        print("\n=== è·¨åŸŸæ³›åŒ–æµ‹è¯• ===")
        
        cresci_test = self.test_df[self.test_df['domain'] == 'cresci']
        gender_test = self.test_df[self.test_df['domain'] == 'gender']
        
        results = {}
        
        # Cresci â†’ Cresci (åŒåŸŸBotæ£€æµ‹)
        if len(cresci_test) > 0:
            acc_cresci = self._evaluate_subset(cresci_test, task='bot')
            results['cresci_to_cresci'] = acc_cresci
            print(f"Cresci (åŒåŸŸ) Botæ£€æµ‹å‡†ç¡®ç‡: {acc_cresci:.4f}")
        else:
            print("âš ï¸ CresciåŸŸæµ‹è¯•æ•°æ®ä¸ºç©º")
        
        # Gender â†’ Gender (åŒåŸŸæ€§åˆ«åˆ†ç±»)
        if len(gender_test) > 0:
            acc_gender = self._evaluate_subset(gender_test, task='gender')
            results['gender_to_gender'] = acc_gender
            print(f"Gender (åŒåŸŸ) æ€§åˆ«åˆ†ç±»å‡†ç¡®ç‡: {acc_gender:.4f}")
        else:
            print("âš ï¸ GenderåŸŸæµ‹è¯•æ•°æ®ä¸ºç©º")
        
        return results
    
    def test_adversarial_robustness(self):
        """å¯¹æŠ—æ”»å‡»é²æ£’æ€§æµ‹è¯•"""
        print("\n=== å¯¹æŠ—æ”»å‡»é²æ£’æ€§æµ‹è¯• ===")
        
        attack_types = ['word_swap', 'char_insert']
        results = {}
        
        # åŸå§‹å‡†ç¡®ç‡
        original_acc = self._evaluate_subset(self.test_df)
        results['original'] = original_acc
        print(f"åŸå§‹å‡†ç¡®ç‡: {original_acc:.4f}")
        
        for attack_type in attack_types:
            print(f"\næ”»å‡»ç±»å‹: {attack_type}")
            
            # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
            attacked_df = self.test_df.copy()
            attacked_df['text'] = attacked_df['text'].apply(
                lambda x: self.adversarial_attack(x, attack_type)
            )
            
            # è¯„ä¼°å¯¹æŠ—æ ·æœ¬
            attacked_acc = self._evaluate_subset(attacked_df)
            results[attack_type] = attacked_acc
            
            drop_rate = (original_acc - attacked_acc) / original_acc if original_acc > 0 else 0
            print(f"æ”»å‡»åå‡†ç¡®ç‡: {attacked_acc:.4f}")
            print(f"å‡†ç¡®ç‡ä¸‹é™: {drop_rate:.2%}")
        
        # å¯è§†åŒ–ç»“æœ
        self.plot_robustness_results(results)
        
        return results
    
    def _evaluate_subset(self, df, task='mixed'):
        """
        è¯„ä¼°æ•°æ®å­é›†çš„å‡†ç¡®ç‡ï¼ˆé€‚é…æ–°æ¨¡å‹çš„å‰å‘æ¨ç†ï¼‰
        Args:
            df: å¾…è¯„ä¼°çš„DataFrame
            task: ä»»åŠ¡ç±»å‹ ('mixed', 'bot', 'gender')
        Returns:
            å‡†ç¡®ç‡
        """
        if len(df) == 0:
            return 0.0
        
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for idx, row in tqdm(df.iterrows(), total=len(df), desc="è¯„ä¼°"):
                # æ–‡æœ¬ç¼–ç 
                encoded = self.tokenizer(
                    row['text'],
                    padding='max_length',
                    truncation=True,
                    max_length=self.config.MAX_LENGTH,
                    return_tensors='pt'
                ).to(self.device)
                
                # æ¨¡å‹æ¨ç†ï¼ˆé€‚é…æ–°æ¨¡å‹çš„forwardå‚æ•°ï¼‰
                outputs = self.model(
                    input_ids=encoded['input_ids'],
                    attention_mask=encoded['attention_mask'],
                    domain=[row['domain']]  # ä¿æŒä¸è®­ç»ƒæ—¶ä¸€è‡´çš„domainå‚æ•°æ ¼å¼
                )
                
                # æ ¹æ®åŸŸ/ä»»åŠ¡é€‰æ‹©åˆ†ç±»å™¨
                if row['domain'] == 'cresci' or task == 'bot':
                    pred = outputs['bot_logits'].argmax(dim=1).item()
                else:
                    pred = outputs['gender_logits'].argmax(dim=1).item()
                
                predictions.append(pred)
                true_labels.append(int(row['label']))
        
        accuracy = accuracy_score(true_labels, predictions)
        return accuracy
    
    def plot_robustness_results(self, results):
        """å¯è§†åŒ–é²æ£’æ€§æµ‹è¯•ç»“æœ"""
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # ç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºï¼ˆæ ¹æ®ç³»ç»Ÿè°ƒæ•´ï¼‰
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        labels = ['original','word_swap','char_insert'] if len(results) == 3 else list(results.keys())
        values = list(results.values())
        
        colors = ['#2E86AB', '#A23B72', '#F18F01']
        bars = ax.bar(labels, values, color=colors[:len(labels)], alpha=0.8)
        
        ax.set_ylabel('ACC', fontsize=12)  # ä¿®æ­£æ‹¼å†™é”™è¯¯ ACU â†’ ACC
        ax.set_title('Robustness Test Results', fontsize=14, fontweight='bold')
        ax.set_ylim([0, 1])
        ax.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.2%}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        save_path = os.path.join(self.config.OUTPUT_PATH, 'robustness_test.png')
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ“ é²æ£’æ€§æµ‹è¯•å›¾å·²ä¿å­˜: {save_path}")
    
    def generate_confusion_matrices(self):
        """ç”Ÿæˆå¹¶ä¿å­˜æ··æ·†çŸ©é˜µï¼ˆé€‚é…æ–°æ¨¡å‹ï¼‰"""
        print("\n=== ç”Ÿæˆæ··æ·†çŸ©é˜µ ===")
        
        cresci_test = self.test_df[self.test_df['domain'] == 'cresci']
        gender_test = self.test_df[self.test_df['domain'] == 'gender']
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Botæ£€æµ‹æ··æ·†çŸ©é˜µ
        if len(cresci_test) > 0:
            predictions = []
            true_labels = []
            
            with torch.no_grad():
                for idx, row in cresci_test.iterrows():
                    encoded = self.tokenizer(
                        row['text'],
                        padding='max_length',
                        truncation=True,
                        max_length=self.config.MAX_LENGTH,
                        return_tensors='pt'
                    ).to(self.device)
                    
                    outputs = self.model(
                        input_ids=encoded['input_ids'],
                        attention_mask=encoded['attention_mask'],
                        domain=['cresci']
                    )
                    
                    pred = outputs['bot_logits'].argmax(dim=1).item()
                    predictions.append(pred)
                    true_labels.append(int(row['label']))
            
            cm = confusion_matrix(true_labels, predictions)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
                       xticklabels=['Human', 'Bot'], yticklabels=['Human', 'Bot'])
            axes[0].set_title('Bot Detection Confusion Matrix', fontsize=12, fontweight='bold')
            axes[0].set_xlabel('Predicted Label')
            axes[0].set_ylabel('True Label')
        else:
            axes[0].text(0.5, 0.5, 'No Cresci Domain Data', ha='center', va='center', transform=axes[0].transAxes)
            axes[0].set_title('Bot Detection Confusion Matrix', fontsize=12, fontweight='bold')
        
        # æ€§åˆ«åˆ†ç±»æ··æ·†çŸ©é˜µ
        if len(gender_test) > 0:
            predictions = []
            true_labels = []
            
            with torch.no_grad():
                for idx, row in gender_test.iterrows():
                    encoded = self.tokenizer(
                        row['text'],
                        padding='max_length',
                        truncation=True,
                        max_length=self.config.MAX_LENGTH,
                        return_tensors='pt'
                    ).to(self.device)
                    
                    outputs = self.model(
                        input_ids=encoded['input_ids'],
                        attention_mask=encoded['attention_mask'],
                        domain=['gender']
                    )
                    
                    pred = outputs['gender_logits'].argmax(dim=1).item()
                    predictions.append(pred)
                    true_labels.append(int(row['label']))
            
            cm = confusion_matrix(true_labels, predictions)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[1],
                       xticklabels=['Male', 'Female', 'Brand'], 
                       yticklabels=['Male', 'Female', 'Brand'])
            axes[1].set_title('Gender Classification Confusion Matrix', fontsize=12, fontweight='bold')
            axes[1].set_xlabel('Predicted Label')
            axes[1].set_ylabel('True Label')
        else:
            axes[1].text(0.5, 0.5, 'No Gender Domain Data', ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title('Gender Classification Confusion Matrix', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        save_path = os.path.join(self.config.OUTPUT_PATH, 'confusion_matrices.png')
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„é²æ£’æ€§æµ‹è¯•æµç¨‹"""
        print("=" * 60)
        print("æ­¥éª¤5: é²æ£’æ€§éªŒè¯")
        print("=" * 60)
        
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        cross_domain_results = self.test_cross_domain_accuracy()
        adversarial_results = self.test_adversarial_robustness()
        self.generate_confusion_matrices()
        
        print("\n" + "=" * 60)
        print("âœ… æ­¥éª¤5å®Œæˆ!")
        print("=" * 60)
        
        # è¿”å›æµ‹è¯•ç»“æœ
        return {
            'cross_domain': cross_domain_results,
            'adversarial': adversarial_results
        }

# ä¸»å‡½æ•°å…¥å£
if __name__ == "__main__":
    from config import Config
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œæµ‹è¯•
    evaluator = RobustnessEvaluator(config)
    test_results = evaluator.run()
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"è·¨åŸŸæµ‹è¯•ç»“æœ: {test_results['cross_domain']}")
    print(f"å¯¹æŠ—æ”»å‡»æµ‹è¯•ç»“æœ: {test_results['adversarial']}")