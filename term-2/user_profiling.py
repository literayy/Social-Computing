import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from collections import Counter
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ==================== åŒæ­¥è®­ç»ƒä»£ç çš„æ¨¡å‹ç»“æ„ ====================
class GradientReversalFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha
        return output, None

class GradientReversalLayer(nn.Module):
    def __init__(self, alpha=1.0):
        super().__init__()
        self.alpha = alpha
    
    def forward(self, x):
        return GradientReversalFunction.apply(x, self.alpha)
    
    def set_alpha(self, alpha):
        self.alpha = alpha

class DomainAdaptiveDeBERTa(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        from transformers import DebertaV2Model
        self.deberta = DebertaV2Model.from_pretrained(config.DEBERTA_PATH)
        self.hidden_size = self.deberta.config.hidden_size
        print(f"âœ“ DeBERTa hidden_size: {self.hidden_size}")
        
        # åˆå§‹åŒ–GRLï¼ˆä½¿ç”¨configä¸­çš„å‚æ•°ï¼‰
        self.grl = GradientReversalLayer(alpha=config.GRL_ALPHA)
        
        # å¢å¼ºç‰ˆåŸŸåˆ†ç±»å™¨ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        self.domain_classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, config.NUM_DOMAINS)
        )
        
        # Botåˆ†ç±»å™¨
        self.bot_classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, config.NUM_BOT_CLASSES)
        )
        
        # å¢å¼ºç‰ˆæ€§åˆ«åˆ†ç±»å™¨ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        self.gender_classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, config.NUM_GENDER_CLASSES)
        )
    
    def forward(self, input_ids, attention_mask, domain):
        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)
        features = outputs.last_hidden_state[:, 0, :]
        
        reversed_features = self.grl(features)
        domain_logits = self.domain_classifier(reversed_features)
        bot_logits = self.bot_classifier(features)
        gender_logits = self.gender_classifier(features)
        
        return {
            'features': features,
            'domain_logits': domain_logits,
            'bot_logits': bot_logits,
            'gender_logits': gender_logits
        }

# ==================== ç”¨æˆ·ç”»åƒä¸»ç±»ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ====================
class UserProfiler:
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config.device_type)
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹å’ŒTokenizer
        self.model = self._load_trained_model()
        self.tokenizer = self._load_tokenizer()
        
        # åˆå§‹åŒ–å˜é‡
        self.user_embeddings = None
        self.user_df = None
        self.cluster_labels = None
        self.cluster_profiles = None
        self.cluster_metrics = {}  # å­˜å‚¨å¤šç»´åº¦èšç±»æŒ‡æ ‡
        
        # ä¼˜åŒ–å‚æ•°ï¼ˆå¯æ ¹æ®ç»“æœè°ƒæ•´ï¼‰
        self.OPTIMIZE_NUM_CLUSTERS = True  # è‡ªåŠ¨ä¼˜åŒ–èšç±»æ•°é‡
        self.CLUSTER_RANGE = [4, 6, 8, 10]  # å€™é€‰èšç±»æ•°é‡
        self.USE_HIERARCHICAL_CLUSTERING = False  # å±‚çº§èšç±»ï¼ˆå¯é€‰ï¼‰
        self.FEATURE_SCALING = True  # ç‰¹å¾æ ‡å‡†åŒ–
        
    def _load_tokenizer(self):
        from transformers import DebertaV2Tokenizer
        try:
            tokenizer = DebertaV2Tokenizer.from_pretrained(self.config.DEBERTA_PATH)
            print("âœ“ TokenizeråŠ è½½æˆåŠŸ")
            return tokenizer
        except Exception as e:
            raise Exception(f"TokenizeråŠ è½½å¤±è´¥: {e}\nè¯·æ£€æŸ¥DEBERTA_PATHé…ç½®: {self.config.DEBERTA_PATH}")
    
    def _load_trained_model(self):
        print("\n=== åŠ è½½è®­ç»ƒæ¨¡å‹ ===")
        model_path = os.path.join(self.config.MODEL_SAVE_PATH, 'best_model.pt')
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\nè¯·æ£€æŸ¥MODEL_SAVE_PATHé…ç½®: {self.config.MODEL_SAVE_PATH}")
        
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        
        model = DomainAdaptiveDeBERTa(self.config).to(self.device)
        
        # å®¹é”™åŠ è½½æƒé‡
        def load_state_dict_with_adjustment(model, state_dict):
            model_dict = model.state_dict()
            filtered_state_dict = {}
            for k, v in state_dict.items():
                if k in model_dict and model_dict[k].shape == v.shape:
                    filtered_state_dict[k] = v
                else:
                    print(f"âš ï¸  è·³è¿‡ä¸åŒ¹é…çš„æƒé‡: {k}")
            model_dict.update(filtered_state_dict)
            model.load_state_dict(model_dict)
            print(f"âœ“ æˆåŠŸåŠ è½½ {len(filtered_state_dict)}/{len(state_dict)} ä¸ªåŒ¹é…çš„æƒé‡å‚æ•°")
        
        load_state_dict_with_adjustment(model, state_dict)
        model.eval()
        print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
        return model
    
    def extract_user_embeddings(self):
        print("\n=== æå–ç”¨æˆ·embeddings ===")
        
        # åŠ è½½é¢„å¤„ç†æ•°æ®
        def load_data(file_path, desc):
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"{desc}æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            df = pd.read_csv(file_path)
            df = df[df['text'].str.len() >= self.config.MIN_TEXT_LENGTH].reset_index(drop=True)
            print(f"âœ“ {desc}æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡")
            
            if len(df) > self.config.PROFILING_SAMPLE_SIZE:
                df = df.sample(n=self.config.PROFILING_SAMPLE_SIZE, random_state=self.config.RANDOM_SEED)
                print(f"âœ“ {desc}æ•°æ®é‡‡æ ·è‡³: {len(df)} æ¡")
            return df
        
        cresci_df = load_data(self.config.PREPROCESSED_CRESCI, "Cresci")
        gender_df = load_data(self.config.PREPROCESSED_GENDER, "Gender")
        
        # æ·»åŠ åŸŸæ ‡ç­¾
        cresci_df['domain'] = 'cresci'
        gender_df['domain'] = 'gender'
        all_df = pd.concat([cresci_df, gender_df], ignore_index=True)
        print(f"âœ“ åˆå¹¶åç”¨æˆ·æ€»æ•°: {len(all_df)}")
        
        # æå–embeddings
        embeddings = []
        batch_size = self.config.BATCH_SIZE
        
        with torch.no_grad():
            for i in tqdm(range(0, len(all_df), batch_size), desc="æå–embeddings"):
                batch_df = all_df.iloc[i:i+batch_size]
                batch_texts = batch_df['text'].tolist()
                
                encoded = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=self.config.MAX_LENGTH,
                    return_tensors='pt'
                ).to(self.device)
                
                outputs = self.model(
                    input_ids=encoded['input_ids'],
                    attention_mask=encoded['attention_mask'],
                    domain=batch_df['domain'].tolist()
                )
                
                batch_embeddings = outputs['features'].cpu().numpy()
                embeddings.append(batch_embeddings)
                
                del encoded, outputs
                if self.config.device_type == 'cuda':
                    torch.cuda.empty_cache()
        
        self.user_embeddings = np.vstack(embeddings)
        self.user_df = all_df.iloc[:len(self.user_embeddings)].reset_index(drop=True)
        print(f"âœ“ Embeddingsæå–å®Œæˆï¼Œshape: {self.user_embeddings.shape}")
        return self.user_embeddings
    
    def _select_best_cluster_num(self, embeddings_pca):
        """è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜èšç±»æ•°é‡ï¼ˆåŸºäºè½®å»“ç³»æ•°+DBIï¼‰"""
        print("\n=== è‡ªåŠ¨ä¼˜åŒ–èšç±»æ•°é‡ ===")
        best_metrics = {
            'n_clusters': self.config.NUM_CLUSTERS,
            'silhouette': -1,
            'davies_bouldin': 999,
            'calinski_harabasz': 0
        }
        
        for n_clusters in self.CLUSTER_RANGE:
            if n_clusters >= len(embeddings_pca):
                continue
            
            # è®­ç»ƒK-means
            kmeans = KMeans(n_clusters=n_clusters, random_state=self.config.RANDOM_SEED, n_init=10)
            labels = kmeans.fit_predict(embeddings_pca)
            
            # è®¡ç®—å¤šç»´åº¦æŒ‡æ ‡
            try:
                silhouette = silhouette_score(embeddings_pca, labels)
                davies_bouldin = davies_bouldin_score(embeddings_pca, labels)
                calinski_harabasz = calinski_harabasz_score(embeddings_pca, labels)
                
                print(f"K={n_clusters}: è½®å»“ç³»æ•°={silhouette:.4f}, DBI={davies_bouldin:.4f}, CHæŒ‡æ•°={calinski_harabasz:.2f}")
                
                # ç»¼åˆè¯„åˆ†ï¼ˆè½®å»“ç³»æ•°è¶Šé«˜+DBIè¶Šä½è¶Šå¥½ï¼‰
                score = silhouette - (davies_bouldin / 10)  # å½’ä¸€åŒ–æƒé‡
                
                # æ›´æ–°æœ€ä¼˜å€¼
                if score > (best_metrics['silhouette'] - (best_metrics['davies_bouldin'] / 10)):
                    best_metrics = {
                        'n_clusters': n_clusters,
                        'silhouette': silhouette,
                        'davies_bouldin': davies_bouldin,
                        'calinski_harabasz': calinski_harabasz,
                        'labels': labels
                    }
            except:
                continue
        
        print(f"\nâœ… æœ€ä¼˜èšç±»æ•°é‡: K={best_metrics['n_clusters']}")
        print(f"   æœ€ä¼˜æŒ‡æ ‡: è½®å»“ç³»æ•°={best_metrics['silhouette']:.4f}, DBI={best_metrics['davies_bouldin']:.4f}")
        return best_metrics['n_clusters'], best_metrics['labels'], best_metrics
    
    def perform_clustering(self):
        """ä¼˜åŒ–ç‰ˆèšç±»æµç¨‹"""
        print("\n=== å±‚æ¬¡åŒ–èšç±»ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ===")
        
        if self.user_embeddings is None:
            raise ValueError("è¯·å…ˆæå–ç”¨æˆ·embeddings")
        
        # æ­¥éª¤1: ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆæå‡èšç±»æ•ˆæœï¼‰
        embeddings_scaled = self.user_embeddings
        if self.FEATURE_SCALING:
            scaler = StandardScaler()
            embeddings_scaled = scaler.fit_transform(self.user_embeddings)
            print("âœ“ ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
        
        # æ­¥éª¤2: PCAé™ç»´ï¼ˆä¼˜åŒ–ç»´åº¦é€‰æ‹©ï¼‰
        n_components = min(50, embeddings_scaled.shape[1], len(embeddings_scaled)-1)
        if n_components < 2:
            n_components = 2
        
        pca = PCA(n_components=n_components, random_state=self.config.RANDOM_SEED)
        embeddings_pca = pca.fit_transform(embeddings_scaled)
        print(f"âœ“ PCAé™ç»´å®Œæˆ: ä¿ç•™ç»´åº¦={embeddings_pca.shape[1]}, æ–¹å·®ä¿ç•™={pca.explained_variance_ratio_.sum():.2%}")
        
        # æ­¥éª¤3: é€‰æ‹©èšç±»æ•°é‡å¹¶æ‰§è¡Œèšç±»
        if self.OPTIMIZE_NUM_CLUSTERS:
            n_clusters, self.cluster_labels, self.cluster_metrics = self._select_best_cluster_num(embeddings_pca)
        else:
            n_clusters = min(self.config.NUM_CLUSTERS, len(embeddings_pca))
            if n_clusters < 2:
                n_clusters = 2
            
            # é€‰æ‹©èšç±»ç®—æ³•
            if self.USE_HIERARCHICAL_CLUSTERING:
                # å±‚çº§èšç±»ï¼ˆæ›´é€‚åˆç”¨æˆ·ç”»åƒçš„æ¨¡ç³Šè¾¹ç•Œï¼‰
                clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
                self.cluster_labels = clustering.fit_predict(embeddings_pca)
                print(f"âœ“ å±‚çº§èšç±»å®Œæˆ (K={n_clusters})")
            else:
                # K-meansèšç±»ï¼ˆä¼˜åŒ–n_initå‚æ•°ï¼‰
                kmeans = KMeans(n_clusters=n_clusters, random_state=self.config.RANDOM_SEED, n_init=20)
                self.cluster_labels = kmeans.fit_predict(embeddings_pca)
                print(f"âœ“ K-meansèšç±»å®Œæˆ (K={n_clusters}, n_init=20)")
            
            # è®¡ç®—å®Œæ•´æŒ‡æ ‡
            try:
                self.cluster_metrics['silhouette'] = silhouette_score(embeddings_pca, self.cluster_labels)
            except:
                self.cluster_metrics['silhouette'] = -1
            
            try:
                self.cluster_metrics['davies_bouldin'] = davies_bouldin_score(embeddings_pca, self.cluster_labels)
            except:
                self.cluster_metrics['davies_bouldin'] = 999
            
            try:
                self.cluster_metrics['calinski_harabasz'] = calinski_harabasz_score(embeddings_pca, self.cluster_labels)
            except:
                self.cluster_metrics['calinski_harabasz'] = 0
        
        # è¾“å‡ºæœ€ç»ˆèšç±»æŒ‡æ ‡
        print(f"\nğŸ“Š æœ€ç»ˆèšç±»è´¨é‡è¯„ä¼°:")
        print(f"  è½®å»“ç³»æ•°: {self.cluster_metrics.get('silhouette', -1):.4f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
        print(f"  Davies-BouldinæŒ‡æ•°: {self.cluster_metrics.get('davies_bouldin', 999):.4f} (è¶Šå°è¶Šå¥½)")
        print(f"  Calinski-HarabaszæŒ‡æ•°: {self.cluster_metrics.get('calinski_harabasz', 0):.2f} (è¶Šå¤§è¶Šå¥½)")
        
        # æ·»åŠ èšç±»æ ‡ç­¾åˆ°ç”¨æˆ·æ•°æ®
        self.user_df['cluster'] = self.cluster_labels
        
        # ç»Ÿè®¡Clusteråˆ†å¸ƒ
        cluster_dist = Counter(self.cluster_labels)
        print(f"\nğŸ“ˆ Clusteråˆ†å¸ƒ:")
        for cluster_id, count in sorted(cluster_dist.items()):
            print(f"  Cluster {cluster_id}: {count} ç”¨æˆ· ({count/len(self.user_df)*100:.1f}%)")
        
        return self.cluster_labels, self.cluster_metrics
    
    def generate_cluster_profiles(self):
        """å¢å¼ºç‰ˆClusterç”»åƒç”Ÿæˆ"""
        print("\n=== ç”ŸæˆClusterç”»åƒï¼ˆå¢å¼ºç‰ˆï¼‰ ===")
        
        if self.cluster_labels is None:
            raise ValueError("è¯·å…ˆæ‰§è¡Œèšç±»")
        
        self.cluster_profiles = {}
        gender_map = {0: 'ç”·æ€§', 1: 'å¥³æ€§', 2: 'å“ç‰Œ', '0': 'ç”·æ€§', '1': 'å¥³æ€§', '2': 'å“ç‰Œ'}
        
        for cluster_id in sorted(Counter(self.cluster_labels).keys()):
            cluster_users = self.user_df[self.user_df['cluster'] == cluster_id]
            if len(cluster_users) == 0:
                continue
            
            # åŸºç¡€åˆ†å¸ƒç»Ÿè®¡
            domain_dist = cluster_users['domain'].value_counts(normalize=True).to_dict()
            label_dist = cluster_users['label'].value_counts(normalize=True).to_dict()
            
            # å¢å¼ºç»Ÿè®¡ï¼šæ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
            text_lengths = cluster_users['text'].str.len()
            text_stats = {
                'avg_length': text_lengths.mean(),
                'std_length': text_lengths.std(),
                'min_length': text_lengths.min(),
                'max_length': text_lengths.max()
            }
            
            # å…³é”®è¯æå–ï¼ˆä¼˜åŒ–TF-IDFï¼‰
            keywords = []
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                vectorizer = TfidfVectorizer(
                    max_features=200,
                    stop_words='english',
                    ngram_range=(1, 2),
                    min_df=2  # è¿‡æ»¤ä½é¢‘è¯
                )
                tfidf_matrix = vectorizer.fit_transform(cluster_users['text'].fillna(''))
                feature_names = vectorizer.get_feature_names_out()
                tfidf_scores = tfidf_matrix.sum(axis=0).A1
                top_indices = tfidf_scores.argsort()[-10:][::-1]
                keywords = [(feature_names[i], tfidf_scores[i]) for i in top_indices if tfidf_scores[i] > 0]
            except Exception as e:
                print(f"âš ï¸  Cluster {cluster_id} å…³é”®è¯æå–å¤±è´¥: {e}")
            
            # æ™ºèƒ½å‘½å
            try:
                main_domain = max(domain_dist, key=domain_dist.get)
                if main_domain == 'cresci':
                    main_label = max(label_dist, key=label_dist.get)
                    cluster_type = "Botç”¨æˆ·" if main_label == 1 else "çœŸå®ç”¨æˆ·"
                else:
                    main_label = max(label_dist, key=label_dist.get)
                    cluster_type = f"{gender_map.get(main_label, 'æœªçŸ¥')}ç”¨æˆ·"
                
                # åŠ å…¥è§„æ¨¡æè¿°
                size_pct = len(cluster_users) / len(self.user_df) * 100
                cluster_name = f"{cluster_type} (å æ¯”{size_pct:.1f}%)"
            except:
                cluster_name = f"æœªçŸ¥ç”¨æˆ·ç¾¤-{cluster_id}"
            
            self.cluster_profiles[cluster_id] = {
                'name': cluster_name,
                'size': len(cluster_users),
                'size_pct': len(cluster_users) / len(self.user_df) * 100,
                'domain_dist': domain_dist,
                'label_dist': label_dist,
                'text_stats': text_stats,
                'keywords': keywords,
                'sample_users': cluster_users.head(5)['text'].tolist()  # æ ·æœ¬æ–‡æœ¬
            }
            
            # è¾“å‡ºè¯¦ç»†ç”»åƒ
            print(f"\n[Cluster {cluster_id}: {cluster_name}]")
            print(f"  è§„æ¨¡: {len(cluster_users)} ç”¨æˆ· ({size_pct:.1f}%)")
            print(f"  åŸŸåˆ†å¸ƒ: {domain_dist}")
            print(f"  æ ‡ç­¾åˆ†å¸ƒ: {label_dist}")
            print(f"  æ–‡æœ¬é•¿åº¦: å¹³å‡{text_stats['avg_length']:.1f}å­— (Â±{text_stats['std_length']:.1f})")
            print(f"  æ ¸å¿ƒå…³é”®è¯: {', '.join([k[0] for k in keywords[:5]]) if keywords else 'æ— '}")
        
        # ä¿å­˜å¢å¼ºç‰ˆç”»åƒ
        profile_path = os.path.join(self.config.OUTPUT_PATH, 'cluster_profiles_enhanced.csv')
        # å±•å¹³å­—å…¸ä»¥ä¾¿ä¿å­˜
        profile_flat = []
        for cid, profile in self.cluster_profiles.items():
            row = {
                'cluster_id': cid,
                'name': profile['name'],
                'size': profile['size'],
                'size_pct': profile['size_pct'],
                'main_domain': max(profile['domain_dist'], key=profile['domain_dist'].get) if profile['domain_dist'] else '',
                'main_label': max(profile['label_dist'], key=profile['label_dist'].get) if profile['label_dist'] else '',
                'avg_text_length': profile['text_stats']['avg_length'],
                'top_keywords': ', '.join([k[0] for k in profile['keywords'][:5]]) if profile['keywords'] else ''
            }
            profile_flat.append(row)
        
        profile_df = pd.DataFrame(profile_flat)
        profile_df.to_csv(profile_path, encoding='utf-8', index=False)
        print(f"\nâœ“ å¢å¼ºç‰ˆç”»åƒä¿å­˜è‡³: {profile_path}")
        
        return self.cluster_profiles
    
    def visualize_clusters(self):
        """ä¼˜åŒ–ç‰ˆt-SNEå¯è§†åŒ–"""
        print("\n=== t-SNEå¯è§†åŒ–ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ===")
        
        if self.user_embeddings is None or self.cluster_labels is None:
            print("âš ï¸  ç¼ºå°‘embeddingsæˆ–èšç±»æ ‡ç­¾ï¼Œè·³è¿‡å¯è§†åŒ–")
            return
        
        # é‡‡æ ·ä¼˜åŒ–
        sample_size = min(1500, len(self.user_embeddings))  # å¢åŠ é‡‡æ ·æ•°é‡æå‡å¯è§†åŒ–æ•ˆæœ
        if sample_size < 10:
            print("âš ï¸  æ ·æœ¬æ•°é‡è¿‡å°‘ï¼Œè·³è¿‡å¯è§†åŒ–")
            return
        
        # å…¼å®¹ä½ç‰ˆæœ¬NumPyçš„éšæœºé‡‡æ ·
        np.random.seed(self.config.RANDOM_SEED)
        sample_indices = np.random.choice(len(self.user_embeddings), sample_size, replace=False)
        
        embeddings_sample = self.user_embeddings[sample_indices]
        cluster_sample = self.cluster_labels[sample_indices]
        domain_sample = self.user_df.iloc[sample_indices]['domain'].tolist()
        
        # t-SNEä¼˜åŒ–ï¼ˆè°ƒæ•´å‚æ•°æå‡æ•ˆæœï¼‰
        try:
            perplexity = min(50, sample_size-1)  # å¢å¤§perplexityæå‡å…¨å±€ç»“æ„
            tsne = TSNE(
                n_components=2,
                random_state=self.config.RANDOM_SEED,
                perplexity=perplexity,
                n_iter=2000,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                learning_rate='auto'
            )
            embeddings_2d = tsne.fit_transform(embeddings_sample)
        except Exception as e:
            print(f"âš ï¸  t-SNEé™ç»´å¤±è´¥: {e}")
            return
        
        # ç»˜åˆ¶ä¼˜åŒ–ç‰ˆå¯è§†åŒ–å›¾
        fig, axes = plt.subplots(1, 3, figsize=(24, 7))  # å¢åŠ ç¬¬ä¸‰ä¸ªå­å›¾ï¼ˆæŒ‰æ ‡ç­¾ç€è‰²ï¼‰
        
        # å­å›¾1: æŒ‰Clusterç€è‰²
        scatter1 = axes[0].scatter(
            embeddings_2d[:, 0], embeddings_2d[:, 1],
            c=cluster_sample, cmap='tab10', alpha=0.8, s=40, edgecolors='white', linewidth=0.5
        )
        axes[0].set_title('User Clustering by Cluster ID', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('t-SNE Component 1')
        axes[0].set_ylabel('t-SNE Component 2')
        plt.colorbar(scatter1, ax=axes[0], label='Cluster ID')
        
        # å­å›¾2: æŒ‰Domainç€è‰²
        domain_colors = [0 if d=='cresci' else 1 for d in domain_sample]
        scatter2 = axes[1].scatter(
            embeddings_2d[:, 0], embeddings_2d[:, 1],
            c=domain_colors, cmap='coolwarm', alpha=0.8, s=40, edgecolors='white', linewidth=0.5
        )
        axes[1].set_title('User Clustering by Domain', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('t-SNE Component 1')
        axes[1].set_ylabel('t-SNE Component 2')
        cbar2 = plt.colorbar(scatter2, ax=axes[1], ticks=[0, 1])
        cbar2.set_ticklabels(['Cresci (Botæ£€æµ‹)', 'Gender (æ€§åˆ«åˆ†ç±»)'])
        
        # å­å›¾3: æŒ‰æ ‡ç­¾ç€è‰²ï¼ˆBot/æ€§åˆ«ï¼‰
        label_sample = self.user_df.iloc[sample_indices]['label'].tolist()
        scatter3 = axes[2].scatter(
            embeddings_2d[:, 0], embeddings_2d[:, 1],
            c=label_sample, cmap='viridis', alpha=0.8, s=40, edgecolors='white', linewidth=0.5
        )
        axes[2].set_title('User Clustering by Label', fontsize=14, fontweight='bold')
        axes[2].set_xlabel('t-SNE Component 1')
        axes[2].set_ylabel('t-SNE Component 2')
        plt.colorbar(scatter3, ax=axes[2], label='Label (0/1/2)')
        
        plt.suptitle(f'User Clustering Visualization (K={len(Counter(self.cluster_labels))})', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        save_path = os.path.join(self.config.OUTPUT_PATH, 'cluster_visualization_enhanced.png')
        plt.savefig(save_path, dpi=200, bbox_inches='tight')
        plt.close()
        print(f"âœ“ å¢å¼ºç‰ˆå¯è§†åŒ–ä¿å­˜è‡³: {save_path}")
    
    def save_results(self):
        """å®Œæ•´ç»“æœä¿å­˜"""
        print("\n=== ä¿å­˜ç»“æœï¼ˆå®Œæ•´ç‰ˆï¼‰ ===")
        
        # 1. ç”¨æˆ·ç”»åƒæ•°æ®
        if self.user_df is not None and len(self.user_df) > 0:
            output_path = os.path.join(self.config.OUTPUT_PATH, 'user_profiles_enhanced.csv')
            self.user_df.to_csv(output_path, index=False, encoding='utf-8')
            print(f"âœ“ ç”¨æˆ·ç”»åƒæ•°æ®: {output_path}")
        
        # 2. Embeddings
        if self.user_embeddings is not None and len(self.user_embeddings) > 0:
            embeddings_path = os.path.join(self.config.OUTPUT_PATH, 'user_embeddings.npy')
            np.save(embeddings_path, self.user_embeddings)
            print(f"âœ“ ç”¨æˆ·embeddings: {embeddings_path}")
        
        # 3. èšç±»æŒ‡æ ‡æŠ¥å‘Š
        metrics_path = os.path.join(self.config.OUTPUT_PATH, 'clustering_metrics_detailed.txt')
        with open(metrics_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("èšç±»è´¨é‡è¯¦ç»†æŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            
            f.write("1. é…ç½®å‚æ•°\n")
            f.write(f"   - èšç±»æ•°é‡: {len(Counter(self.cluster_labels))}\n")
            f.write(f"   - é‡‡æ ·æ•°é‡: {self.config.PROFILING_SAMPLE_SIZE}\n")
            f.write(f"   - ç‰¹å¾æ ‡å‡†åŒ–: {self.FEATURE_SCALING}\n")
            f.write(f"   - PCAç»´åº¦: {min(50, self.user_embeddings.shape[1], len(self.user_embeddings)-1)}\n\n")
            
            f.write("2. æ ¸å¿ƒæŒ‡æ ‡\n")
            f.write(f"   - è½®å»“ç³»æ•° (Silhouette Score): {self.cluster_metrics.get('silhouette', -1):.4f}\n")
            f.write(f"     (è§£è¯»: è¶Šæ¥è¿‘1è¶Šå¥½ï¼Œ0.3+ä¸ºå¯æ¥å—ï¼Œ0.5+ä¸ºè‰¯å¥½)\n")
            f.write(f"   - Davies-BouldinæŒ‡æ•°: {self.cluster_metrics.get('davies_bouldin', 999):.4f}\n")
            f.write(f"     (è§£è¯»: è¶Šå°è¶Šå¥½ï¼Œ<1.5ä¸ºå¯æ¥å—ï¼Œ<1.0ä¸ºè‰¯å¥½)\n")
            f.write(f"   - Calinski-HarabaszæŒ‡æ•°: {self.cluster_metrics.get('calinski_harabasz', 0):.2f}\n")
            f.write(f"     (è§£è¯»: è¶Šå¤§è¶Šå¥½ï¼Œæ•°å€¼è¶Šé«˜è¯´æ˜èšç±»è¶Šç´§å‡‘)\n\n")
            
            f.write("3. Clusteråˆ†å¸ƒ\n")
            cluster_dist = Counter(self.cluster_labels)
            total_users = len(self.user_df)
            for cluster_id, count in sorted(cluster_dist.items()):
                pct = count / total_users * 100
                profile = self.cluster_profiles.get(cluster_id, {})
                name = profile.get('name', f"Cluster {cluster_id}")
                f.write(f"   - {name}: {count} ç”¨æˆ· ({pct:.1f}%)\n")
        
        print(f"âœ“ èšç±»æŒ‡æ ‡æŠ¥å‘Š: {metrics_path}")
        
        # 4. ç”»åƒæ‘˜è¦
        summary_path = os.path.join(self.config.OUTPUT_PATH, 'cluster_summary.md')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# ç”¨æˆ·èšç±»ç”»åƒæ‘˜è¦\n")
            f.write("## èšç±»è´¨é‡\n")
            f.write(f"- è½®å»“ç³»æ•°: {self.cluster_metrics.get('silhouette', -1):.4f}\n")
            f.write(f"- Davies-BouldinæŒ‡æ•°: {self.cluster_metrics.get('davies_bouldin', 999):.4f}\n\n")
            
            f.write("## å„Clusterç‰¹å¾\n")
            for cid, profile in self.cluster_profiles.items():
                f.write(f"### Cluster {cid}: {profile['name']}\n")
                f.write(f"- è§„æ¨¡: {profile['size']} ç”¨æˆ· ({profile['size_pct']:.1f}%)\n")
                f.write(f"- ä¸»è¦åŸŸ: {max(profile['domain_dist'], key=profile['domain_dist'].get) if profile['domain_dist'] else 'æ— '}\n")
                f.write(f"- æ ¸å¿ƒå…³é”®è¯: {', '.join([k[0] for k in profile['keywords'][:5]]) if profile['keywords'] else 'æ— '}\n\n")
        
        print(f"âœ“ ç”»åƒæ‘˜è¦ (Markdown): {summary_path}")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´ä¼˜åŒ–æµç¨‹"""
        print("=" * 60)
        print("æ­¥éª¤3: å±‚æ¬¡åŒ–ç”¨æˆ·ç”»åƒæ„å»ºï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
        print("=" * 60)
        print(f"ä¼˜åŒ–é…ç½®:")
        print(f"  - è‡ªåŠ¨ä¼˜åŒ–èšç±»æ•°é‡: {self.OPTIMIZE_NUM_CLUSTERS}")
        print(f"  - å€™é€‰èšç±»æ•°é‡: {self.CLUSTER_RANGE}")
        print(f"  - ç‰¹å¾æ ‡å‡†åŒ–: {self.FEATURE_SCALING}")
        print(f"  - å±‚çº§èšç±»: {self.USE_HIERARCHICAL_CLUSTERING}")
        
        try:
            # æå–embeddings
            self.extract_user_embeddings()
            
            # ä¼˜åŒ–èšç±»
            cluster_labels, metrics = self.perform_clustering()
            
            # ç”Ÿæˆå¢å¼ºç‰ˆç”»åƒ
            self.generate_cluster_profiles()
            
            # ä¼˜åŒ–å¯è§†åŒ–
            self.visualize_clusters()
            
            # ä¿å­˜å®Œæ•´ç»“æœ
            self.save_results()
            
            print("\n" + "=" * 60)
            print("âœ… æ­¥éª¤3å®Œæˆ! (ä¼˜åŒ–ç‰ˆ)")
            print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {self.config.OUTPUT_PATH}")
            print("=" * 60)
            
        except Exception as e:
            print(f"\nâŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None
        
        return self.user_df, self.cluster_profiles


if __name__ == "__main__":
    from config import Config
    
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    print(config)
    
    # åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆProfiler
    profiler = UserProfiler(config)
    
    # å¯é€‰ï¼šè°ƒæ•´ä¼˜åŒ–å‚æ•°
    # profiler.OPTIMIZE_NUM_CLUSTERS = False  # å…³é—­è‡ªåŠ¨ä¼˜åŒ–ï¼Œä½¿ç”¨é…ç½®çš„NUM_CLUSTERS
    # profiler.USE_HIERARCHICAL_CLUSTERING = True  # å¯ç”¨å±‚çº§èšç±»
    # profiler.CLUSTER_RANGE = [5, 7, 9]  # è°ƒæ•´å€™é€‰èšç±»æ•°é‡
    
    # æ‰§è¡Œæµç¨‹
    user_df, profiles = profiler.run()