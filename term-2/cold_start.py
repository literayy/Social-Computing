import torch
import pandas as pd
import numpy as np
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import accuracy_score, classification_report
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import os
from tqdm import tqdm
import gc


class ColdStartRecommender:
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆé€‚é…æ–°ç‰ˆDomainAdaptiveDeBERTaï¼‰
        self.model = self._load_trained_model()
        self.tokenizer = self._load_tokenizer()
        
        # ç”»åƒæ•°æ®ç›¸å…³ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šä¼˜å…ˆåŠ è½½UserProfilerçš„å¢å¼ºç‰ˆæ•°æ®ï¼‰
        self.user_profiles = None
        self.user_embeddings = None
        self.actual_cluster_num = 0  # åŠ¨æ€è®°å½•å®é™…èšç±»æ•°é‡
        self.cluster_mapping = {}    # æ˜ å°„KNNç´¢å¼•åˆ°å®é™…cluster ID
        self._load_or_build_user_profiles()
        
        # æ„å»ºåŸå‹
        self.prototypes = None
        self.knn_model = None
        
    def _load_tokenizer(self):
        """åŠ è½½DeBERTaV2åˆ†è¯å™¨ï¼ˆé€‚é…æ–°ç‰ˆæ¨¡å‹ï¼‰"""
        from transformers import DebertaV2Tokenizer
        tokenizer = DebertaV2Tokenizer.from_pretrained(self.config.DEBERTA_PATH)
        return tokenizer
    
    def _load_trained_model(self):
        """åŠ è½½æ–°ç‰ˆDomainAdaptiveDeBERTaæ¨¡å‹"""
        # å¯¼å…¥æ–°ç‰ˆæ¨¡å‹ç±»
        from domain_adaptation import DomainAdaptiveDeBERTa
        
        model_path = os.path.join(self.config.MODEL_SAVE_PATH, 'best_model.pt')
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹: {model_path}")
        
        # åŠ è½½æ¨¡å‹æƒé‡ï¼ˆé€‚é…æ–°ç‰ˆä¿å­˜æ ¼å¼ï¼‰
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        model = DomainAdaptiveDeBERTa(self.config).to(self.device)
        
        # å…¼å®¹ä¸åŒä¿å­˜æ ¼å¼
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        model.load_state_dict(state_dict)
        model.eval()
        
        # å…³é—­GRLï¼ˆæ¨ç†é˜¶æ®µæ— éœ€æ¢¯åº¦åè½¬ï¼‰
        model.grl.set_alpha(0.0)
        
        print(f"âœ“ åŠ è½½æ–°ç‰ˆæ¨¡å‹: {model_path}")
        print(f"âœ“ æ¨¡å‹è®¾å¤‡: {self.device}")
        return model
    
    def _generate_user_embeddings(self):
        """ç”Ÿæˆç”¨æˆ·embeddingï¼ˆé€‚é…æ–°ç‰ˆæ¨¡å‹å¤šä»»åŠ¡è¾“å‡ºï¼‰"""
        print("\n=== ç”Ÿæˆç”¨æˆ·ç”»åƒembedding ===")
        
        # åŠ è½½å…¨é‡æ•°æ®
        if os.path.exists(os.path.join(self.config.OUTPUT_PATH, "val_set.csv")):
            df = pd.read_csv(os.path.join(self.config.OUTPUT_PATH, "val_set.csv"))
        else:
            # åŠ è½½åŸå§‹é¢„å¤„ç†æ•°æ®
            cresci_df = pd.read_csv(self.config.PREPROCESSED_CRESCI)
            gender_df = pd.read_csv(self.config.PREPROCESSED_GENDER)
            df = pd.concat([cresci_df, gender_df], ignore_index=True)
        
        # æ•°æ®è¿‡æ»¤
        df = df[df['text'].str.len() >= self.config.MIN_TEXT_LENGTH].reset_index(drop=True)
        df = df.sample(n=min(5000, len(df)), random_state=42)  # é‡‡æ ·å‡å°‘è®¡ç®—é‡
        
        # ç”Ÿæˆembedding
        embeddings = []
        user_ids = []
        texts = []
        domains = []
        labels = []
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = 32
        for i in tqdm(range(0, len(df), batch_size), desc="ç”Ÿæˆembedding"):
            batch_df = df.iloc[i:i+batch_size]
            
            # ç¼–ç æ–‡æœ¬
            encoded = self.tokenizer(
                batch_df['text'].tolist(),
                padding=True,
                truncation=True,
                max_length=self.config.MAX_LENGTH,
                return_tensors='pt'
            ).to(self.device)
            
            # å‰å‘æ¨ç†ï¼ˆé€‚é…æ–°ç‰ˆæ¨¡å‹è¾“å‡ºï¼‰
            with torch.no_grad():
                outputs = self.model(
                    input_ids=encoded['input_ids'],
                    attention_mask=encoded['attention_mask'],
                    domain=batch_df['domain'].tolist()  # ä¼ å…¥domainåˆ—è¡¨
                )
                batch_embeddings = outputs['features'].cpu().numpy()
            
            # æ”¶é›†ç»“æœ
            embeddings.extend(batch_embeddings)
            user_ids.extend(batch_df.get('user_id', range(i, i+len(batch_df))).tolist())
            texts.extend(batch_df['text'].tolist())
            domains.extend(batch_df['domain'].tolist())
            labels.extend(batch_df['label'].tolist())
            
            # å†…å­˜æ¸…ç†
            del encoded, outputs, batch_embeddings
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        
        # è½¬æ¢ä¸ºæ•°ç»„
        self.user_embeddings = np.array(embeddings)
        
        # èšç±»ç”Ÿæˆclusteræ ‡ç­¾ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šåŠ¨æ€é€‚é…èšç±»æ•°é‡ï¼‰
        print("\n=== å¯¹ç”¨æˆ·embeddingè¿›è¡ŒKMeansèšç±» ===")
        # å…ˆå°è¯•åŠ è½½UserProfilerçš„èšç±»æŒ‡æ ‡
        metrics_path = os.path.join(self.config.OUTPUT_PATH, 'clustering_metrics_detailed.txt')
        if os.path.exists(metrics_path):
            # è¯»å–UserProfileré€‰æ‹©çš„æœ€ä¼˜Kå€¼
            with open(metrics_path, 'r', encoding='utf-8') as f:
                content = f.read()
                import re
                k_match = re.search(r'èšç±»æ•°é‡: (\d+)', content)
                if k_match:
                    self.actual_cluster_num = int(k_match.group(1))
                    print(f"âœ“ ä»UserProfilerè¯»å–æœ€ä¼˜èšç±»æ•°é‡: {self.actual_cluster_num}")
        
        # è‹¥æœªè¯»å–åˆ°ï¼Œåˆ™ä½¿ç”¨é…ç½®å€¼ï¼ˆé™çº§ç­–ç•¥ï¼‰
        if self.actual_cluster_num <= 1:
            self.actual_cluster_num = self.config.NUM_CLUSTERS
            print(f"âš ï¸  æœªæ‰¾åˆ°UserProfilerèšç±»ç»“æœï¼Œä½¿ç”¨é…ç½®å€¼: {self.actual_cluster_num}")
        
        # æ‰§è¡Œèšç±»
        kmeans = KMeans(n_clusters=self.actual_cluster_num, random_state=42)
        clusters = kmeans.fit_predict(self.user_embeddings)
        
        # æ„å»ºç”¨æˆ·ç”»åƒæ•°æ®
        self.user_profiles = pd.DataFrame({
            'user_id': user_ids,
            'text': texts,
            'domain': domains,
            'label': labels,
            'cluster': clusters
        })
        
        # ä¿å­˜ç”»åƒæ•°æ®
        os.makedirs(self.config.OUTPUT_PATH, exist_ok=True)
        self.user_profiles.to_csv(os.path.join(self.config.OUTPUT_PATH, 'user_profiles.csv'), index=False)
        np.save(os.path.join(self.config.OUTPUT_PATH, 'user_embeddings.npy'), self.user_embeddings)
        
        print(f"âœ“ ç”Ÿæˆ {len(self.user_profiles)} æ¡ç”¨æˆ·ç”»åƒ")
        print(f"âœ“ å®é™…èšç±»æ•°é‡: {self.actual_cluster_num}")
        print(f"âœ“ Clusteråˆ†å¸ƒ: {pd.Series(clusters).value_counts().to_dict()}")
    
    def _load_or_build_user_profiles(self):
        """æ ¸å¿ƒä¿®æ”¹ï¼šä¼˜å…ˆåŠ è½½UserProfilerçš„å¢å¼ºç‰ˆæ•°æ®"""
        # ä¼˜å…ˆåŠ è½½UserProfilerç”Ÿæˆçš„å¢å¼ºç‰ˆæ•°æ®
        profile_paths = [
            os.path.join(self.config.OUTPUT_PATH, 'user_profiles_enhanced.csv'),
            os.path.join(self.config.OUTPUT_PATH, 'user_profiles.csv')
        ]
        embedding_path = os.path.join(self.config.OUTPUT_PATH, 'user_embeddings.npy')
        
        # å¯»æ‰¾å¯ç”¨çš„ç”»åƒæ–‡ä»¶
        profile_path = None
        for p in profile_paths:
            if os.path.exists(p):
                profile_path = p
                break
        
        if profile_path and os.path.exists(embedding_path):
            # åŠ è½½å·²æœ‰æ•°æ®ï¼ˆæ ¸å¿ƒï¼šåŠ¨æ€è·å–å®é™…èšç±»æ•°é‡ï¼‰
            self.user_profiles = pd.read_csv(profile_path)
            self.user_embeddings = np.load(embedding_path)
            
            # åŠ¨æ€ç»Ÿè®¡å®é™…èšç±»æ•°é‡å’ŒID
            actual_clusters = sorted(self.user_profiles['cluster'].unique())
            self.actual_cluster_num = len(actual_clusters)
            # æ„å»ºç´¢å¼•æ˜ å°„ï¼ˆKNNç´¢å¼• â†’ å®é™…cluster IDï¼‰
            self.cluster_mapping = {i: cid for i, cid in enumerate(actual_clusters)}
            
            print(f"âœ“ åŠ è½½å·²æœ‰ç”¨æˆ·ç”»åƒ: {len(self.user_profiles)} æ¡")
            print(f"âœ“ å®é™…èšç±»æ•°é‡: {self.actual_cluster_num} (cluster ID: {actual_clusters})")
        else:
            # ç”Ÿæˆæ–°æ•°æ®
            self._generate_user_embeddings()
            # ç”Ÿæˆåé‡æ–°ç»Ÿè®¡
            actual_clusters = sorted(self.user_profiles['cluster'].unique())
            self.cluster_mapping = {i: cid for i, cid in enumerate(actual_clusters)}
    
    def build_prototypes(self):
        """æ ¸å¿ƒä¿®æ”¹ï¼šåŠ¨æ€é€‚é…å®é™…èšç±»æ•°é‡ï¼Œè·³è¿‡ç©ºcluster"""
        print("\n=== æ„å»ºClusteråŸå‹ ===")
        print(f"ğŸ“Œ é€‚é…å®é™…èšç±»æ•°é‡: {self.actual_cluster_num}")
        
        self.prototypes = {}
        prototype_domains = {}  # è®°å½•æ¯ä¸ªclusterçš„ä¸»è¦åŸŸ
        
        # æ ¸å¿ƒä¿®æ”¹ï¼šéå†å®é™…å­˜åœ¨çš„cluster IDï¼Œè€Œéé…ç½®çš„NUM_CLUSTERS
        actual_clusters = sorted(self.user_profiles['cluster'].unique())
        for cluster_id in actual_clusters:
            cluster_mask = self.user_profiles['cluster'] == cluster_id
            cluster_embeddings = self.user_embeddings[cluster_mask]
            cluster_domains = self.user_profiles.loc[cluster_mask, 'domain']
            
            if len(cluster_embeddings) == 0:
                print(f"âš ï¸ Cluster {cluster_id} æ— æ•°æ®ï¼Œè·³è¿‡")
                continue
            
            # åŸå‹ = clusterçš„å‡å€¼å‘é‡
            prototype = cluster_embeddings.mean(axis=0)
            self.prototypes[cluster_id] = prototype
            
            # ç»Ÿè®¡clusterçš„åŸŸåˆ†å¸ƒ
            domain_dist = cluster_domains.value_counts()
            main_domain = domain_dist.index[0] if len(domain_dist) > 0 else 'unknown'
            prototype_domains[cluster_id] = main_domain
            
            print(f"Cluster {cluster_id}: {cluster_mask.sum()} ç”¨æˆ· | ä¸»è¦åŸŸ: {main_domain} | åŸå‹å‘é‡ {prototype.shape}")
        
        # æ„å»ºKNNæ¨¡å‹ï¼ˆæ ¸å¿ƒï¼šåŸºäºå®é™…æœ‰æ•ˆclusterï¼‰
        valid_clusters = list(self.prototypes.keys())
        if not valid_clusters:
            raise ValueError("æ— æœ‰æ•ˆclusterï¼Œæ— æ³•æ„å»ºKNNæ¨¡å‹")
        
        # æ›´æ–°æ˜ å°„å…³ç³»ï¼ˆKNNç´¢å¼• â†’ å®é™…cluster IDï¼‰
        self.cluster_mapping = {i: cid for i, cid in enumerate(valid_clusters)}
        prototype_matrix = np.vstack([self.prototypes[i] for i in valid_clusters])
        
        self.knn_model = NearestNeighbors(n_neighbors=3, metric='cosine')
        self.knn_model.fit(prototype_matrix)
        
        # ä¿å­˜cluster-åŸŸæ˜ å°„
        self.prototype_domains = prototype_domains
        print(f"âœ“ åŸå‹æ„å»ºå®Œæˆï¼ˆæœ‰æ•ˆcluster: {len(valid_clusters)}ï¼‰ï¼ŒKNNæ¨¡å‹å°±ç»ª")
        print(f"âœ“ KNNç´¢å¼•æ˜ å°„: {self.cluster_mapping}")
    
    def predict_cold_start_user(self, user_texts, domain='cresci'):
        """æ ¸å¿ƒä¿®æ”¹ï¼šä¿®å¤KNNç´¢å¼•åˆ°å®é™…cluster IDçš„æ˜ å°„"""
        # æå–ç”¨æˆ·embedding
        with torch.no_grad():
            encoded = self.tokenizer(
                user_texts,
                padding=True,
                truncation=True,
                max_length=self.config.MAX_LENGTH,
                return_tensors='pt'
            ).to(self.device)
            
            # é€‚é…æ–°ç‰ˆæ¨¡å‹ï¼šä¼ å…¥domainå‚æ•°
            outputs = self.model(
                input_ids=encoded['input_ids'],
                attention_mask=encoded['attention_mask'],
                domain=[domain] * len(user_texts)
            )
            
            # å–å¤šæ¡æ¨æ–‡çš„å¹³å‡embedding
            user_embedding = outputs['features'].cpu().numpy().mean(axis=0, keepdims=True)
        
        # KNNæ£€ç´¢æœ€è¿‘çš„åŸå‹ï¼ˆæ ¸å¿ƒï¼šä½¿ç”¨æ˜ å°„å…³ç³»è½¬æ¢ç´¢å¼•ï¼‰
        distances, indices = self.knn_model.kneighbors(user_embedding)
        
        # å…³é”®ä¿®å¤ï¼šå°†KNNè¿”å›çš„ç´¢å¼•è½¬æ¢ä¸ºå®é™…cluster ID
        predicted_index = indices[0][0]
        predicted_cluster = self.cluster_mapping.get(predicted_index, -1)
        main_domain = self.prototype_domains.get(predicted_cluster, 'unknown')
        
        if predicted_cluster == -1:
            print(f"âš ï¸  é¢„æµ‹ç´¢å¼• {predicted_index} æ— å¯¹åº”cluster ID")
        
        return predicted_cluster, distances[0][0], main_domain
    
    def evaluate_cold_start(self):
        """è¯„ä¼°å†·å¯åŠ¨æ•ˆæœï¼ˆé€‚é…åŠ¨æ€èšç±»æ•°é‡ï¼‰"""
        print("\n=== è¯„ä¼°å†·å¯åŠ¨æ•ˆæœ ===")
        
        # æ¨¡æ‹Ÿå†·å¯åŠ¨åœºæ™¯ï¼šéšæœºé‡‡æ ·ç”¨æˆ·ï¼Œåªç”¨å‰Næ¡æ¨æ–‡
        test_sizes = [1, 3, 5, 10]
        results = {
            'overall': {},
            'cresci': {},
            'gender': {}
        }
        
        # é‡‡æ ·æµ‹è¯•ç”¨æˆ·ï¼ˆæŒ‰åŸŸåˆ†å±‚ï¼‰
        test_users = self.user_profiles.groupby('domain').apply(
            lambda x: x.sample(n=min(100, len(x)), random_state=42)
        ).reset_index(drop=True)
        
        for n_tweets in test_sizes:
            print(f"\nä½¿ç”¨å‰ {n_tweets} æ¡æ¨æ–‡...")
            
            # æŒ‰åŸŸç»Ÿè®¡é¢„æµ‹ç»“æœ
            predictions = {'overall': [], 'cresci': [], 'gender': []}
            true_labels = {'overall': [], 'cresci': [], 'gender': []}
            
            for idx, row in tqdm(test_users.iterrows(), total=len(test_users), desc=f"æµ‹è¯•"):
                # æ¨¡æ‹Ÿï¼šåªå–æ–‡æœ¬çš„å‰n_tweetsä¸ªå¥å­
                text = row['text']
                sentences = text.split('.')[:n_tweets]
                truncated_text = ['. '.join(sentences)]
                
                # é¢„æµ‹ï¼ˆä¼ å…¥ç”¨æˆ·æ‰€å±åŸŸï¼‰
                pred_cluster, _, _ = self.predict_cold_start_user(
                    truncated_text, 
                    domain=row['domain']
                )
                
                # è¿‡æ»¤æ— æ•ˆé¢„æµ‹
                if pred_cluster == -1:
                    continue
                
                # æŒ‰åŸŸè®°å½•ç»“æœ
                domain = row['domain']
                predictions['overall'].append(pred_cluster)
                true_labels['overall'].append(row['cluster'])
                
                if domain in predictions:
                    predictions[domain].append(pred_cluster)
                    true_labels[domain].append(row['cluster'])
            
            # è®¡ç®—å„åŸŸå‡†ç¡®ç‡ï¼ˆå¤„ç†ç©ºåˆ—è¡¨ï¼‰
            def safe_accuracy(true, pred):
                if len(true) == 0 or len(pred) == 0:
                    return 0.0
                return accuracy_score(true, pred)
            
            results['overall'][n_tweets] = safe_accuracy(true_labels['overall'], predictions['overall'])
            results['cresci'][n_tweets] = safe_accuracy(true_labels['cresci'], predictions['cresci'])
            results['gender'][n_tweets] = safe_accuracy(true_labels['gender'], predictions['gender'])
            
            # æ‰“å°ç»“æœ
            print(f"æ•´ä½“å‡†ç¡®ç‡: {results['overall'][n_tweets]:.4f}")
            if 'cresci' in results and n_tweets in results['cresci']:
                print(f"CresciåŸŸå‡†ç¡®ç‡: {results['cresci'][n_tweets]:.4f}")
            if 'gender' in results and n_tweets in results['gender']:
                print(f"GenderåŸŸå‡†ç¡®ç‡: {results['gender'][n_tweets]:.4f}")
        
        # å¯è§†åŒ–å†·å¯åŠ¨æ•ˆæœï¼ˆæ–°å¢å¤šåŸŸå¯¹æ¯”ï¼‰
        self.plot_cold_start_results(results)
        
        return results
    
    def plot_cold_start_results(self, results):
        """ç»˜åˆ¶å†·å¯åŠ¨æ•ˆæœæ›²çº¿ï¼ˆå¤šåŸŸå¯¹æ¯”ï¼‰"""
        fig, ax = plt.subplots(figsize=(12, 7))
        
        # ç»˜åˆ¶ä¸åŒåŸŸçš„æ›²çº¿
        colors = {'overall': '#2E86AB', 'cresci': '#A23B72', 'gender': '#F18F01'}
        markers = {'overall': 'o', 'cresci': 's', 'gender': '^'}
        
        for domain, color in colors.items():
            if domain in results and results[domain]:
                x = list(results[domain].keys())
                y = list(results[domain].values())
                
                ax.plot(x, y, 
                        marker=markers[domain], 
                        linewidth=2, 
                        markersize=10, 
                        color=color,
                        label=f'{domain.capitalize()} Domain')
                ax.fill_between(x, y, alpha=0.2, color=color)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for xi, yi in zip(x, y):
                    ax.text(xi, yi + 0.02, f'{yi:.2%}', ha='center', fontsize=9, color=color)
        
        # æ ¸å¿ƒä¿®æ”¹ï¼šæ ‡é¢˜æ˜¾ç¤ºå®é™…èšç±»æ•°é‡
        ax.set_xlabel('Number of Tweets', fontsize=12)
        ax.set_ylabel('User Profile Prediction Accuracy', fontsize=12)
        ax.set_title(f'Cold-start Performance (K={self.actual_cluster_num})', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0, 1])
        ax.legend(fontsize=10)
        
        plt.tight_layout()
        save_path = os.path.join(self.config.OUTPUT_PATH, 'cold_start_performance.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ“ å†·å¯åŠ¨æ•ˆæœå›¾: {save_path}")
        plt.close()
    
    def demo_recommendation(self):
        """æ¼”ç¤ºï¼šåŸºäºç”»åƒæ¨èç›¸ä¼¼ç”¨æˆ·ï¼ˆé€‚é…åŠ¨æ€èšç±»æ•°é‡ï¼‰"""
        print("\n=== æ¨èç³»ç»ŸDemoï¼ˆå¤šåŸŸé€‚é…ï¼‰===")
        
        # æŒ‰åŸŸåˆ†å±‚é€‰æ‹©ç¤ºä¾‹ç”¨æˆ·
        for domain in ['cresci', 'gender']:
            domain_users = self.user_profiles[self.user_profiles['domain'] == domain]
            if len(domain_users) == 0:
                continue
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªç”¨æˆ·
            sample_user = domain_users.sample(n=1).iloc[0]
            user_idx = sample_user.name
            user_cluster = sample_user['cluster']
            user_embedding = self.user_embeddings[user_idx:user_idx+1]
            
            print(f"\nã€{domain.upper()} åŸŸç¤ºä¾‹ã€‘")
            print(f"ç›®æ ‡ç”¨æˆ·: {sample_user['user_id']}")
            print(f"æ‰€å±Cluster: {user_cluster} (å®é™…èšç±»æ•°é‡: {self.actual_cluster_num})")
            print(f"æ–‡æœ¬ç‰‡æ®µ: {sample_user['text'][:100]}...")
            
            # æ¨èåŒclusterçš„ç›¸ä¼¼ç”¨æˆ·ï¼ˆåŒåŸŸä¼˜å…ˆï¼‰
            cluster_users = self.user_profiles[
                (self.user_profiles['cluster'] == user_cluster) & 
                (self.user_profiles['domain'] == domain)
            ]
            
            if len(cluster_users) < 6:  # ä¸è¶³åˆ™æ”¾å®½åŸŸé™åˆ¶
                cluster_users = self.user_profiles[self.user_profiles['cluster'] == user_cluster]
            
            if len(cluster_users) < 2:
                print(f"âš ï¸  Cluster {user_cluster} ç›¸ä¼¼ç”¨æˆ·ä¸è¶³ï¼Œè·³è¿‡")
                continue
            
            cluster_indices = cluster_users.index.tolist()
            cluster_embeddings = self.user_embeddings[cluster_indices]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(user_embedding, cluster_embeddings)[0]
            
            # Top 5æ¨èï¼ˆæ’é™¤è‡ªå·±ï¼‰
            top_indices = np.argsort(similarities)[::-1][1:6]
            if len(top_indices) < 1:
                print(f"âš ï¸  ç›¸ä¼¼ç”¨æˆ·ä¸è¶³ï¼Œè·³è¿‡")
                continue
            
            recommended_users = cluster_users.iloc[top_indices]
            
            print(f"æ¨èçš„ç›¸ä¼¼ç”¨æˆ· (Top {len(top_indices)}):")
            for i, (idx, row) in enumerate(recommended_users.iterrows(), 1):
                print(f"{i}. User {row['user_id']} (ç›¸ä¼¼åº¦: {similarities[top_indices[i-1]]:.4f})")
                print(f"   åŸŸ: {row['domain']} | æ ‡ç­¾: {row['label']}")
                print(f"   æ–‡æœ¬: {row['text'][:80]}...")
                print("   ---")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´å†·å¯åŠ¨æµç¨‹"""
        print("=" * 60)
        print("æ­¥éª¤4: å†·å¯åŠ¨åŸå‹å­¦ä¹ ï¼ˆé€‚é…åŠ¨æ€èšç±»æ•°é‡ï¼‰")
        print("=" * 60)
        print(f"ğŸ“Œ å…³é”®é…ç½®:")
        print(f"   - é…ç½®èšç±»æ•°é‡: {self.config.NUM_CLUSTERS}")
        print(f"   - å®é™…èšç±»æ•°é‡: {self.actual_cluster_num}")
        
        # æ„å»ºåŸå‹
        self.build_prototypes()
        
        # è¯„ä¼°å†·å¯åŠ¨
        results = self.evaluate_cold_start()
        
        # Demoæ¨è
        self.demo_recommendation()
        
        print("\n" + "=" * 60)
        print("âœ… æ­¥éª¤4å®Œæˆ!")
        print("=" * 60)
        
        return results


if __name__ == "__main__":
    from config import Config
    
    config = Config()
    # è¡¥å……å†·å¯åŠ¨ç›¸å…³é…ç½®ï¼ˆè‹¥Configä¸­æœªå®šä¹‰ï¼‰
    if not hasattr(config, 'NUM_CLUSTERS'):
        config.NUM_CLUSTERS = 8  # ä»…ä½œä¸ºé™çº§é»˜è®¤å€¼
    if not hasattr(config, 'MAX_LENGTH'):
        config.MAX_LENGTH = 512
    if not hasattr(config, 'MIN_TEXT_LENGTH'):
        config.MIN_TEXT_LENGTH = 10
    
    recommender = ColdStartRecommender(config)
    results = recommender.run()